{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount(\"/content/drive\")\n",
        "    # Si quieres, permite override con variable de entorno\n",
        "    PROJECT_ROOT = Path(os.environ.get(\n",
        "        \"AI_PROJECT_ROOT\",\n",
        "        \"/content/drive/MyDrive/Colab Notebooks/AI Project\"\n",
        "    )).resolve()\n",
        "else:\n",
        "    # Local: asumir que el notebook estÃ¡ en backend/notebooks/\n",
        "    cwd = Path.cwd().resolve()\n",
        "\n",
        "    # Si estÃ¡s parado dentro de \"notebooks\", sube 1 nivel (backend/)\n",
        "    PROJECT_ROOT = cwd.parent if cwd.name.lower() == \"notebooks\" else cwd\n",
        "\n",
        "    # Si por alguna razÃ³n lo abren desde el root del zip que contiene backend/\n",
        "    if (PROJECT_ROOT / \"backend\").exists() and (PROJECT_ROOT / \"backend\" / \"artifacts\").exists():\n",
        "        PROJECT_ROOT = (PROJECT_ROOT / \"backend\").resolve()\n",
        "\n",
        "DATA_DIR   = PROJECT_ROOT / \"data\"\n",
        "ART_DIR    = PROJECT_ROOT / \"artifacts\"\n",
        "GEO_DIR    = PROJECT_ROOT / \"assets\" / \"geo\"\n",
        "OUT_DIR    = PROJECT_ROOT / \"outputs\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"IN_COLAB:\", IN_COLAB)\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"ART_DIR:\", ART_DIR)\n",
        "print(\"GEO_DIR:\", GEO_DIR)\n",
        "print(\"OUT_DIR:\", OUT_DIR)"
      ],
      "metadata": {
        "id": "5y0naNruaB0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CONFIGURACIÃ“N\n",
        "# =========================\n",
        "RISK_RATE = 0.30\n",
        "SAMPLE_SIZE = 30000\n",
        "\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Carga de datos\n",
        "# =========================\n",
        "\n",
        "train_path = DATA_DIR / \"train_18_24_pca95_target30_NO_SMOTE.csv\"\n",
        "test_path  = DATA_DIR / \"test_25_pca95_target30_NO_SMOTE.csv\"\n",
        "\n",
        "if (not train_path.exists()) or (not test_path.exists()):\n",
        "    print(\"âš ï¸ No se encontraron CSV de entrenamiento en:\")\n",
        "    print(\" -\", train_path)\n",
        "    print(\" -\", test_path)\n",
        "    print(\"\\nâž¡ï¸ Se omite entrenamiento. Se intentarÃ¡ cargar el bundle ya entrenado desde artifacts/.\\n\")\n",
        "\n",
        "    pkl_existing = ART_DIR / \"best_model_bundle.pkl\"\n",
        "    if pkl_existing.exists():\n",
        "        best_bundle = joblib.load(pkl_existing)\n",
        "        print(\"âœ… Bundle cargado:\", pkl_existing)\n",
        "        print(\"Contenido bundle:\", {k: best_bundle.get(k) for k in [\"model_type\", \"model_name\", \"risk_rate\", \"threshold\"]})\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            \"No hay CSV de entrenamiento y tampoco existe artifacts/best_model_bundle.pkl.\\n\"\n",
        "            \"SoluciÃ³n: incluye los CSV (aunque sean pequeÃ±os) o incluye el bundle entrenado en artifacts/.\"\n",
        "        )\n",
        "\n",
        "else:\n",
        "\n",
        "  train_df = pd.read_csv(train_path)\n",
        "  test_df  = pd.read_csv(test_path)\n",
        "\n",
        "  pc_cols = [c for c in train_df.columns if c.startswith(\"PC\")]\n",
        "  print(\"Num PCs:\", len(pc_cols))\n",
        "\n",
        "  X_train_full = train_df[pc_cols].astype(np.float32)\n",
        "  y_train_full = train_df[\"Target\"].astype(int)\n",
        "\n",
        "  X_test = test_df[pc_cols].astype(np.float32)\n",
        "  y_test = test_df[\"Target\"].astype(int)\n",
        "\n",
        "  X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "      X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
        "  )\n",
        "\n",
        "  print(\"Train distribution:\", y_train_full.value_counts(normalize=True).to_dict())\n",
        "  print(\"Test distribution :\", y_test.value_counts(normalize=True).to_dict())\n",
        "\n",
        "  # Sample para SVM y KNN\n",
        "  if len(X_train_full) > SAMPLE_SIZE:\n",
        "      idx = np.random.RandomState(42).choice(len(X_train_full), SAMPLE_SIZE, replace=False)\n",
        "      X_train_sample = X_train_full.iloc[idx]\n",
        "      y_train_sample = y_train_full.iloc[idx]\n",
        "  else:\n",
        "      X_train_sample = X_train_full\n",
        "      y_train_sample = y_train_full\n",
        "\n",
        "  # =========================\n",
        "  # Modelos sklearn (sin XGBoost aquÃ­)\n",
        "  # =========================\n",
        "  modelos = {\n",
        "      \"RegresiÃ³n LogÃ­stica\": LogisticRegression(\n",
        "          solver=\"lbfgs\", max_iter=3000, class_weight=\"balanced\"\n",
        "      ),\n",
        "\n",
        "      \"SVM (LinearSVC Calibrado)\": CalibratedClassifierCV(\n",
        "          estimator=LinearSVC(\n",
        "              C=0.5,\n",
        "              class_weight=\"balanced\",\n",
        "              max_iter=10000,\n",
        "              tol=1e-3,\n",
        "              dual=False,\n",
        "              random_state=42\n",
        "          ),\n",
        "          method=\"sigmoid\",\n",
        "          cv=3\n",
        "      ),\n",
        "\n",
        "      \"KNN\": KNeighborsClassifier(\n",
        "          n_neighbors=15, weights=\"distance\", n_jobs=-1\n",
        "      ),\n",
        "\n",
        "      \"Random Forest\": RandomForestClassifier(\n",
        "          n_estimators=200,\n",
        "          max_depth=18,\n",
        "          min_samples_leaf=5,\n",
        "          max_features=\"sqrt\",\n",
        "          n_jobs=-1,\n",
        "          random_state=42,\n",
        "          class_weight=\"balanced_subsample\"\n",
        "      ),\n",
        "  }\n",
        "\n",
        "  # =========================\n",
        "  # Funciones\n",
        "  # =========================\n",
        "  def get_scores(model, X):\n",
        "      if hasattr(model, \"predict_proba\"):\n",
        "          return model.predict_proba(X)[:, 1]\n",
        "      if hasattr(model, \"decision_function\"):\n",
        "          s = model.decision_function(X)\n",
        "          s = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
        "          return s\n",
        "      raise ValueError(\"El modelo no soporta predict_proba ni decision_function.\")\n",
        "\n",
        "  def threshold_topk(scores, risk_rate=0.30):\n",
        "      return float(np.quantile(scores, 1 - risk_rate))\n",
        "\n",
        "  def evaluar_y_plot(nombre, y_score, y_true, risk_rate):\n",
        "      thr = threshold_topk(y_score, risk_rate=risk_rate)\n",
        "      y_pred = (y_score >= thr).astype(int)\n",
        "\n",
        "      acc = accuracy_score(y_true, y_pred)\n",
        "      prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "      rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "      f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "      auc = roc_auc_score(y_true, y_score)\n",
        "      pr_auc = average_precision_score(y_true, y_score)\n",
        "\n",
        "      thr_info = f\"TOP {int(risk_rate*100)}% en TEST (thr={thr:.3f})\"\n",
        "\n",
        "      cm = confusion_matrix(y_true, y_pred)\n",
        "      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "      fig, ax = plt.subplots(figsize=(5,4))\n",
        "      disp.plot(ax=ax, values_format=\"d\")\n",
        "      ax.set_title(f\"Matriz de ConfusiÃ³n - {nombre}\\n{thr_info}\")\n",
        "      plt.show()\n",
        "\n",
        "      row = {\n",
        "          \"Modelo\": nombre,\n",
        "          \"Umbral\": thr,\n",
        "          \"Umbral_criterio\": thr_info,\n",
        "          \"Accuracy\": acc,\n",
        "          \"Precision\": prec,\n",
        "          \"Recall\": rec,\n",
        "          \"F1-Score\": f1,\n",
        "          \"AUC-ROC\": auc,\n",
        "          \"PR-AUC\": pr_auc\n",
        "      }\n",
        "      return row\n",
        "\n",
        "  # =========================\n",
        "  # Entrenamiento + selecciÃ³n mejor modelo (PR-AUC)\n",
        "  # =========================\n",
        "  resultados = []\n",
        "  best_pr_auc = -1.0\n",
        "  best_name = None\n",
        "  best_obj = None\n",
        "  best_bundle = None\n",
        "  best_metrics = None\n",
        "  xgb_evals_result = None\n",
        "  xgb_model_path = None\n",
        "\n",
        "  # ---- sklearn models ----\n",
        "  for nombre, modelo in modelos.items():\n",
        "      print(f\"\\n--- Entrenando {nombre} ---\")\n",
        "\n",
        "      if nombre in [\"SVM (LinearSVC Calibrado)\", \"KNN\"]:\n",
        "          modelo.fit(X_train_sample, y_train_sample)\n",
        "      else:\n",
        "          modelo.fit(X_train_full, y_train_full)\n",
        "\n",
        "      y_test_score = get_scores(modelo, X_test)\n",
        "      row = evaluar_y_plot(nombre, y_test_score, y_test, RISK_RATE)\n",
        "      resultados.append(row)\n",
        "\n",
        "      if row[\"PR-AUC\"] > best_pr_auc:\n",
        "          best_pr_auc = row[\"PR-AUC\"]\n",
        "          best_name = nombre\n",
        "          best_obj = modelo\n",
        "          best_metrics = row.copy()\n",
        "          best_bundle = {\n",
        "              \"model_type\": \"sklearn\",\n",
        "              \"model_name\": nombre,\n",
        "              \"model\": modelo,\n",
        "              \"threshold\": row[\"Umbral\"],\n",
        "              \"pc_cols\": pc_cols,\n",
        "              \"risk_rate\": RISK_RATE\n",
        "          }\n",
        "\n",
        "  # ---- XGBoost con xgb.train() (con early stopping compatible) ----\n",
        "  print(\"\\n--- Entrenando XGBoost (xgb.train con early stopping) ---\")\n",
        "\n",
        "  dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "  dval   = xgb.DMatrix(X_val, label=y_val)\n",
        "  dtest  = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "  params = {\n",
        "      \"objective\": \"binary:logistic\",\n",
        "      \"eval_metric\": \"aucpr\",\n",
        "      \"learning_rate\": 0.05,\n",
        "      \"max_depth\": 3,\n",
        "      \"subsample\": 0.8,\n",
        "      \"colsample_bytree\": 0.8,\n",
        "      \"min_child_weight\": 5,\n",
        "      \"lambda\": 2.0,\n",
        "      \"alpha\": 0.1,\n",
        "      \"tree_method\": \"hist\",\n",
        "      \"seed\": 42\n",
        "  }\n",
        "\n",
        "  xgb_evals_result = {}\n",
        "  bst = xgb.train(\n",
        "      params=params,\n",
        "      dtrain=dtrain,\n",
        "      num_boost_round=1500,\n",
        "      evals=[(dval, \"val\")],\n",
        "      early_stopping_rounds=50,\n",
        "      evals_result=xgb_evals_result,\n",
        "      verbose_eval=False\n",
        "  )\n",
        "\n",
        "  y_test_score_xgb = bst.predict(dtest)\n",
        "  row_xgb = evaluar_y_plot(\"XGBoost\", y_test_score_xgb, y_test, RISK_RATE)\n",
        "  resultados.append(row_xgb)\n",
        "\n",
        "  # SelecciÃ³n por PR-AUC (incluyendo XGBoost)\n",
        "  if row_xgb[\"PR-AUC\"] > best_pr_auc:\n",
        "      best_pr_auc = row_xgb[\"PR-AUC\"]\n",
        "      best_name = \"XGBoost\"\n",
        "      best_obj = bst\n",
        "      best_metrics = row_xgb.copy()\n",
        "\n",
        "      # Guarda el modelo real de XGBoost como JSON (modelo, no metadata)\n",
        "      xgb_model_path = ART_DIR / \"best_xgboost_model.json\"\n",
        "      bst.save_model(xgb_model_path)\n",
        "\n",
        "      best_bundle = {\n",
        "          \"model_type\": \"xgboost_booster\",\n",
        "          \"model_name\": \"XGBoost\",\n",
        "          \"threshold\": row_xgb[\"Umbral\"],\n",
        "          \"pc_cols\": pc_cols,\n",
        "          \"risk_rate\": RISK_RATE,\n",
        "          \"xgb_model_path\": xgb_model_path\n",
        "      }\n",
        "\n",
        "  # Ranking final por PR-AUC\n",
        "  res_df = pd.DataFrame(resultados).sort_values(\"PR-AUC\", ascending=False).reset_index(drop=True)\n",
        "  print(\"\\n===== Ranking (por PR-AUC) =====\")\n",
        "  display(res_df)\n",
        "\n",
        "  print(f\"\\nâœ… Mejor modelo por PR-AUC: {best_name} (PR-AUC={best_pr_auc:.4f})\")\n",
        "\n",
        "  # =========================\n",
        "  # Guardar mejor bundle (.pkl) + metadata (.json)\n",
        "  # =========================\n",
        "  pkl_path = ART_DIR / \"best_model_bundle.pkl\"\n",
        "  joblib.dump(best_bundle, pkl_path)\n",
        "  print(f\"ðŸ’¾ Bundle (PKL) guardado en: {pkl_path}\")\n",
        "\n",
        "  json_payload = {\n",
        "      \"model_name\": best_name,\n",
        "      \"model_type\": best_bundle.get(\"model_type\"),\n",
        "      \"threshold\": float(best_bundle[\"threshold\"]),\n",
        "      \"risk_rate\": float(best_bundle[\"risk_rate\"]),\n",
        "      \"pc_cols\": list(best_bundle[\"pc_cols\"]),\n",
        "      \"best_metrics\": {k: (float(v) if isinstance(v, (np.floating, float, int)) else v)\n",
        "                      for k, v in best_metrics.items() if k != \"Modelo\"},\n",
        "      \"xgb_model_path\": best_bundle.get(\"xgb_model_path\", None)\n",
        "  }\n",
        "\n",
        "  json_path = ART_DIR / \"best_model_bundle.json\"\n",
        "  with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "      json.dump(json_payload, f, ensure_ascii=False, indent=2)\n",
        "  print(f\"ðŸ§¾ Metadata (JSON) guardado en: {json_path}\")\n",
        "\n",
        "  # =========================\n",
        "  # Curva tipo \"Ã©pocas\" (XGBoost) -> logloss y aucpr en validaciÃ³n\n",
        "  # =========================\n",
        "  if xgb_evals_result is not None:\n",
        "      if \"val\" in xgb_evals_result:\n",
        "          if \"logloss\" in xgb_evals_result[\"val\"]:\n",
        "              plt.figure(figsize=(7,4))\n",
        "              plt.plot(xgb_evals_result[\"val\"][\"logloss\"], label=\"val logloss\")\n",
        "              plt.title(\"XGBoost: logloss por boosting round (tipo Ã©pocas)\")\n",
        "              plt.xlabel(\"Boosting round\")\n",
        "              plt.ylabel(\"logloss\")\n",
        "              plt.legend()\n",
        "              plt.show()\n",
        "\n",
        "          if \"aucpr\" in xgb_evals_result[\"val\"]:\n",
        "              plt.figure(figsize=(7,4))\n",
        "              plt.plot(xgb_evals_result[\"val\"][\"aucpr\"], label=\"val aucpr\")\n",
        "              plt.title(\"XGBoost: AUC-PR por boosting round (tipo Ã©pocas)\")\n",
        "              plt.xlabel(\"Boosting round\")\n",
        "              plt.ylabel(\"aucpr\")\n",
        "              plt.legend()\n",
        "              plt.show()\n"
      ],
      "metadata": {
        "id": "xtp3gu5Xc_Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "# --- paths (ajusta si lo mueves a repo) ---\n",
        "RAW_GEO_DIR = DATA_DIR / \"raw_geo\"\n",
        "\n",
        "PATH_CANT_SHP = RAW_GEO_DIR / \"cantones\" / \"nxcantones.shp\"\n",
        "PATH_PROV_SHP = RAW_GEO_DIR / \"provincias\" / \"LIMITE_PROVINCIAL_CONALI_CNE_2022.shp\"\n",
        "\n",
        "OUT_CANT_GEOJSON = GEO_DIR / \"cantones.geojson\"\n",
        "OUT_PROV_GEOJSON = GEO_DIR / \"provincias.geojson\"\n",
        "\n",
        "GEO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if (not PATH_CANT_SHP.exists()) or (not PATH_PROV_SHP.exists()):\n",
        "    print(\"â„¹ï¸ No se encontraron shapefiles (.shp) en:\")\n",
        "    print(\" -\", PATH_CANT_SHP)\n",
        "    print(\" -\", PATH_PROV_SHP)\n",
        "    print(\"\\nâž¡ï¸ Se omite conversiÃ³n SHP->GeoJSON.\")\n",
        "    print(\"âœ… Se usarÃ¡n los GeoJSON existentes en assets/geo/ (deben estar en el zip).\")\n",
        "\n",
        "    # ValidaciÃ³n Ãºtil para el profesor:\n",
        "    if not OUT_CANT_GEOJSON.exists():\n",
        "        raise FileNotFoundError(f\"Falta {OUT_CANT_GEOJSON}. InclÃºyelo en el zip en assets/geo/.\")\n",
        "    if not OUT_PROV_GEOJSON.exists():\n",
        "        raise FileNotFoundError(f\"Falta {OUT_PROV_GEOJSON}. InclÃºyelo en el zip en assets/geo/.\")\n",
        "\n",
        "    print(\"âœ… GeoJSON listos:\")\n",
        "    print(\" -\", OUT_CANT_GEOJSON)\n",
        "    print(\" -\", OUT_PROV_GEOJSON)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Caso 2: SÃ­ hay SHP -> convertir y exportar GeoJSON\n",
        "# ----------------------------------------------------------\n",
        "else:\n",
        "  print(\"âœ… Shapefiles encontrados. Convirtiendo a GeoJSON...\")\n",
        "\n",
        "  # --- Cantones ---\n",
        "  cant = gpd.read_file(PATH_CANT_SHP)\n",
        "\n",
        "  # a web-friendly CRS\n",
        "  cant = cant.to_crs(\"EPSG:4326\")\n",
        "\n",
        "  # normaliza tipos/cÃ³digos (strings con ceros a la izquierda)\n",
        "  cant[\"DPA_CANTON\"] = cant[\"DPA_CANTON\"].astype(str).str.zfill(4)\n",
        "  cant[\"DPA_PROVIN\"] = cant[\"DPA_PROVIN\"].astype(str).str.zfill(2)\n",
        "\n",
        "  # deja solo lo necesario (reduce peso)\n",
        "  cant = cant[[\"DPA_CANTON\", \"DPA_DESCAN\", \"DPA_PROVIN\", \"DPA_DESPRO\", \"geometry\"]]\n",
        "\n",
        "  # Simplificar geometrÃ­a para que pese menos\n",
        "  cant[\"geometry\"] = cant[\"geometry\"].simplify(tolerance=0.001, preserve_topology=True)\n",
        "\n",
        "  cant.to_file(OUT_CANT_GEOJSON, driver=\"GeoJSON\")\n",
        "\n",
        "  # --- Provincias ---\n",
        "  prov = gpd.read_file(PATH_PROV_SHP).to_crs(\"EPSG:4326\")\n",
        "\n",
        "  # asegÃºrate de tener CODPRO\n",
        "  prov[\"CODPRO\"] = prov[\"CODPRO\"].astype(str).str.zfill(2)\n",
        "\n",
        "  # (opcional) conserva tambiÃ©n nombre si existe (cambia NOMPRO/DPA_DESPRO segÃºn tu shapefile)\n",
        "  cols = [\"CODPRO\"]\n",
        "  for candidate in [\"NOMPRO\", \"DPA_DESPRO\", \"PROVINCIA\", \"NOMBRE\"]:\n",
        "      if candidate in prov.columns:\n",
        "          cols.append(candidate)\n",
        "          break\n",
        "\n",
        "  prov = prov[cols + [\"geometry\"]]\n",
        "\n",
        "  #Simplificar\n",
        "  prov[\"geometry\"] = prov[\"geometry\"].simplify(tolerance=0.001, preserve_topology=True)\n",
        "\n",
        "  prov.to_file(OUT_PROV_GEOJSON, driver=\"GeoJSON\")\n",
        "\n",
        "  print(\"Listo:\", OUT_CANT_GEOJSON, OUT_PROV_GEOJSON)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sT4ZL4QrGxL",
        "outputId": "44a2d767-a5ed-4a06-fed6-766c96d7dcaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listo: /content/drive/MyDrive/Colab Notebooks/AI Project/assets/geo/cantones.geojson /content/drive/MyDrive/Colab Notebooks/AI Project/assets/geo/provincias.geojson\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "import xgboost as xgb\n",
        "\n",
        "# =========================\n",
        "# Paths (PORTABLE)\n",
        "# =========================\n",
        "PREP_PKL  = ART_DIR / \"preprocess_bundle.pkl\"\n",
        "MODEL_PKL = ART_DIR / \"best_model_bundle.pkl\"\n",
        "\n",
        "PROV_GEOJSON = GEO_DIR / \"provincias.geojson\"\n",
        "CANT_GEOJSON = GEO_DIR / \"cantones.geojson\"\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for p in [PREP_PKL, MODEL_PKL, PROV_GEOJSON, CANT_GEOJSON]:\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Falta archivo requerido: {p}\")\n",
        "\n",
        "prep = joblib.load(PREP_PKL)\n",
        "mb   = joblib.load(MODEL_PKL)\n",
        "\n",
        "thr = float(mb[\"threshold\"])\n",
        "model_type = mb.get(\"model_type\", \"sklearn\")\n",
        "\n",
        "print(\"OK preprocess keys:\", prep.keys())\n",
        "print(\"OK model bundle keys:\", mb.keys())\n",
        "print(\"model_type:\", model_type)\n",
        "print(\"Threshold:\", thr)\n",
        "print(\"PCA k:\", len(prep[\"pc_cols\"]))\n",
        "\n",
        "# =========================\n",
        "# Cargar modelo segÃºn tipo\n",
        "# =========================\n",
        "if model_type == \"xgboost_booster\":\n",
        "    xgb_path = mb.get(\"xgb_model_path\", None)\n",
        "    if not xgb_path:\n",
        "        raise ValueError(\"Bundle indica xgboost_booster pero no tiene xgb_model_path\")\n",
        "\n",
        "    xgb_path = Path(xgb_path)\n",
        "\n",
        "    # Si viene como ruta rara, intenta usar artifacts/\n",
        "    if not xgb_path.exists():\n",
        "        candidate = ART_DIR / xgb_path.name\n",
        "        if candidate.exists():\n",
        "            xgb_path = candidate\n",
        "\n",
        "    booster = xgb.Booster()\n",
        "    booster.load_model(str(xgb_path))\n",
        "    model = booster\n",
        "else:\n",
        "    if \"model\" not in mb:\n",
        "        raise ValueError(\"Bundle tipo sklearn pero no contiene la clave 'model'\")\n",
        "    model = mb[\"model\"]\n",
        "\n",
        "def preprocesar_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    cols_a_eliminar = []\n",
        "    fugas_data = [\n",
        "        \"codigo\", \"amie\", \"fex_\", \"puntuacion\", \"inev\", \"imat\", \"ilyl\", \"icn\", \"ies\",\n",
        "        \"ihis\", \"ifil\", \"ied\", \"ifis\", \"iqui\", \"ibio\", \"nl_\"\n",
        "    ]\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col == \"Target\":\n",
        "            continue\n",
        "        if any(key in col.lower() for key in fugas_data):\n",
        "            cols_a_eliminar.append(col)\n",
        "        elif df[col].dtype == \"object\" and df[col].nunique(dropna=True) > 100 and \"id_\" not in col.lower():\n",
        "            cols_a_eliminar.append(col)\n",
        "\n",
        "    df = df.drop(columns=list(set(cols_a_eliminar)), errors=\"ignore\")\n",
        "\n",
        "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
        "        df[col] = df[col].astype(\"category\")\n",
        "    for col in [\"id_prov\", \"id_cant\", \"grado\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    df = pd.get_dummies(df, drop_first=True)\n",
        "    df = df.fillna(0)\n",
        "    return df.astype(\"float32\")\n",
        "\n",
        "def read_csv_smart(path):\n",
        "    # prueba separadores + encodings comunes\n",
        "    seps = [\";\", \",\", \"\\t\", \"|\"]\n",
        "    encs = [\"utf-8\", \"latin1\", \"cp1252\"]\n",
        "    last_err = None\n",
        "\n",
        "    for enc in encs:\n",
        "        for sep in seps:\n",
        "            try:\n",
        "                df = pd.read_csv(path, sep=sep, encoding=enc, low_memory=False, engine=\"python\")\n",
        "                if df.shape[1] > 1:\n",
        "                    return df, sep, enc\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "\n",
        "    df = pd.read_csv(path, sep=None, engine=\"python\", encoding=\"latin1\", on_bad_lines=\"skip\")\n",
        "    return df, None, \"latin1\"\n",
        "\n",
        "def predict_and_export_heat(input_csv: str, out_prefix=\"sest\"):\n",
        "    # 1) cargar crudo\n",
        "    df_raw, sep_used, enc_used = read_csv_smart(input_csv)\n",
        "    print(f\"CSV leÃ­do con sep={sep_used} encoding={enc_used} shape={df_raw.shape}\")\n",
        "\n",
        "    # 2) separar IDs (para agrupar)\n",
        "    # En tu SEST suelen existir id_prov e id_cant\n",
        "    id_prov = df_raw[\"id_prov\"].astype(str).str.zfill(2) if \"id_prov\" in df_raw.columns else None\n",
        "    id_cant = df_raw[\"id_cant\"].astype(str).str.zfill(4) if \"id_cant\" in df_raw.columns else None\n",
        "\n",
        "    # 3) preprocesar (dummies + limpieza)\n",
        "    X = preprocesar_features(df_raw)\n",
        "\n",
        "    # 4) alinear columnas exactamente como en training (1072)\n",
        "    X = X.reindex(columns=prep[\"model_input_cols\"], fill_value=0)\n",
        "\n",
        "    # 5) scaler + pca\n",
        "    X_scaled = prep[\"scaler\"].transform(X)\n",
        "    X_pca = prep[\"pca\"].transform(X_scaled)\n",
        "    X_pca = pd.DataFrame(X_pca, columns=prep[\"pc_cols\"])\n",
        "\n",
        "    # 6) predicciÃ³n\n",
        "    pc_cols_used = mb.get(\"pc_cols\", prep[\"pc_cols\"])\n",
        "    feats = X_pca[mb[\"pc_cols\"]]\n",
        "\n",
        "    if model_type == \"xgboost_booster\":\n",
        "        dtest = xgb.DMatrix(feats.values)\n",
        "        proba = model.predict(dtest)\n",
        "    else:\n",
        "        proba = model.predict_proba(feats)[:, 1]\n",
        "\n",
        "    df_out = df_raw.copy()\n",
        "    df_out[\"risk_score\"] = proba\n",
        "    df_out[\"risk_flag\"]  = (df_out[\"risk_score\"] >= thr).astype(int)\n",
        "\n",
        "    # 7) asegurar ids (para mapa)\n",
        "    if id_prov is not None:\n",
        "        df_out[\"CODPRO\"] = id_prov\n",
        "    if id_cant is not None:\n",
        "        df_out[\"DPA_CANTON\"] = id_cant\n",
        "\n",
        "    # --- outputs ---\n",
        "    pred_path = OUT_DIR / f\"{out_prefix}_predicciones.csv\"\n",
        "    df_out.to_csv(pred_path, index=False)\n",
        "\n",
        "    # 8) agregaciÃ³n provincias\n",
        "    if \"CODPRO\" in df_out.columns:\n",
        "        prov = (df_out.groupby(\"CODPRO\")\n",
        "                .agg(n=(\"risk_score\",\"size\"),\n",
        "                     mean_risk=(\"risk_score\",\"mean\"),\n",
        "                     pct_flag=(\"risk_flag\",\"mean\"))\n",
        "                .reset_index())\n",
        "        prov[\"pct_flag\"] = (prov[\"pct_flag\"]*100).round(2)\n",
        "        prov[\"mean_risk\"] = prov[\"mean_risk\"].round(6)\n",
        "        prov_path = OUT_DIR / f\"{out_prefix}_heat_provincias.json\"\n",
        "        prov.to_json(prov_path, orient=\"records\", force_ascii=False)\n",
        "    else:\n",
        "        prov_path = None\n",
        "\n",
        "    # 9) agregaciÃ³n cantones\n",
        "    if \"CODPRO\" in df_out.columns and \"DPA_CANTON\" in df_out.columns:\n",
        "        cant = (df_out.groupby([\"CODPRO\",\"DPA_CANTON\"])\n",
        "                .agg(n=(\"risk_score\",\"size\"),\n",
        "                     mean_risk=(\"risk_score\",\"mean\"),\n",
        "                     pct_flag=(\"risk_flag\",\"mean\"))\n",
        "                .reset_index())\n",
        "        cant[\"pct_flag\"] = (cant[\"pct_flag\"]*100).round(2)\n",
        "        cant[\"mean_risk\"] = cant[\"mean_risk\"].round(6)\n",
        "        cant_path = OUT_DIR / f\"{out_prefix}_heat_cantones.json\"\n",
        "        cant.to_json(cant_path, orient=\"records\", force_ascii=False)\n",
        "    else:\n",
        "        cant_path = None\n",
        "\n",
        "    meta = {\"threshold\": thr, \"rows\": int(len(df_out))}\n",
        "    meta_path = OUT_DIR / f\"{out_prefix}_meta.json\"\n",
        "    meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    print(\"âœ… Listo outputs:\")\n",
        "    print(\"-\", pred_path)\n",
        "    if prov_path: print(\"-\", prov_path)\n",
        "    if cant_path: print(\"-\", cant_path)\n",
        "    print(\"-\", meta_path)\n",
        "\n",
        "    return df_out\n",
        "\n",
        "example_csv = DATA_DIR / \"examples\" / \"SEST25_micro_example.csv\"\n",
        "if not example_csv.exists():\n",
        "    print(\"âš ï¸ No existe CSV de ejemplo en:\", example_csv)\n",
        "    print(\"âž¡ï¸ Pon un ejemplo en data/examples/ o cambia el path aquÃ­.\")\n",
        "else:\n",
        "    df_pred = predict_and_export_heat(input_csv=str(example_csv), out_prefix=\"SEST25\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGoed6UauWdJ",
        "outputId": "a8828741-6283-431f-f507-8d8a94c004d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK preprocess keys: dict_keys(['model_input_cols', 'scaler', 'pca', 'pc_cols', 'id_cols', 'target_rate', 'low_prop_threshold_t', 'exclude_nl_inev', 'created_at'])\n",
            "OK model keys: dict_keys(['model_type', 'model_name', 'model', 'threshold', 'pc_cols', 'risk_rate'])\n",
            "Threshold: 0.8392292919095269\n",
            "PCA k: 661\n",
            "CSV leÃ­do con sep=None encoding=latin1 shape=(50578, 76)\n",
            "âœ… Listo outputs:\n",
            "- /content/drive/MyDrive/Colab Notebooks/AI Project/outputs/SEST25_predicciones.csv\n",
            "- /content/drive/MyDrive/Colab Notebooks/AI Project/outputs/SEST25_heat_provincias.json\n",
            "- /content/drive/MyDrive/Colab Notebooks/AI Project/outputs/SEST25_heat_cantones.json\n",
            "- /content/drive/MyDrive/Colab Notebooks/AI Project/outputs/SEST25_meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import folium\n",
        "from folium.features import GeoJsonTooltip\n",
        "from pathlib import Path\n",
        "\n",
        "# =========================\n",
        "# RUTAS\n",
        "# =========================\n",
        "\n",
        "PROV_GEOJSON = GEO_DIR / \"provincias.geojson\"\n",
        "CANT_GEOJSON = GEO_DIR / \"cantones.geojson\"\n",
        "\n",
        "HEAT_PROV = OUT_DIR / \"SEST25_heat_provincias.json\"\n",
        "HEAT_CANT = OUT_DIR / \"SEST25_heat_cantones.json\"\n",
        "\n",
        "for p in [PROV_GEOJSON, CANT_GEOJSON, HEAT_PROV, HEAT_CANT]:\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Falta archivo requerido para mapa: {p}\")\n",
        "\n",
        "# =========================\n",
        "# CARGA\n",
        "# =========================\n",
        "gprov = gpd.read_file(PROV_GEOJSON)\n",
        "gcant = gpd.read_file(CANT_GEOJSON)\n",
        "\n",
        "prov_heat = pd.read_json(HEAT_PROV)\n",
        "cant_heat = pd.read_json(HEAT_CANT)\n",
        "\n",
        "# Normalizar llaves\n",
        "if \"CODPRO\" in gprov.columns:\n",
        "    gprov[\"CODPRO\"] = gprov[\"CODPRO\"].astype(str).str.zfill(2)\n",
        "if \"CODPRO\" in prov_heat.columns:\n",
        "    prov_heat[\"CODPRO\"] = prov_heat[\"CODPRO\"].astype(str).str.zfill(2)\n",
        "\n",
        "if \"DPA_CANTON\" in gcant.columns:\n",
        "    gcant[\"DPA_CANTON\"] = gcant[\"DPA_CANTON\"].astype(str).str.zfill(4)\n",
        "if \"DPA_CANTON\" in cant_heat.columns:\n",
        "    cant_heat[\"DPA_CANTON\"] = cant_heat[\"DPA_CANTON\"].astype(str).str.zfill(4)\n",
        "\n",
        "if \"CODPRO\" in gcant.columns:\n",
        "    gcant[\"CODPRO\"] = gcant[\"CODPRO\"].astype(str).str.zfill(2)\n",
        "if \"CODPRO\" in cant_heat.columns:\n",
        "    cant_heat[\"CODPRO\"] = cant_heat[\"CODPRO\"].astype(str).str.zfill(2)\n",
        "\n",
        "# Merge (NO rellenamos NaN -> asÃ­ se ve \"sin datos\" en blanco)\n",
        "gprov_m = gprov.merge(prov_heat, on=\"CODPRO\", how=\"left\") if \"CODPRO\" in gprov.columns else gprov\n",
        "\n",
        "if (\"CODPRO\" in gcant.columns) and (\"CODPRO\" in cant_heat.columns):\n",
        "    gcant_m = gcant.merge(cant_heat, on=[\"CODPRO\", \"DPA_CANTON\"], how=\"left\")\n",
        "else:\n",
        "    gcant_m = gcant.merge(cant_heat, on=\"DPA_CANTON\", how=\"left\")\n",
        "\n",
        "# =========================\n",
        "# Helper: escoger columnas de nombre si existen\n",
        "# =========================\n",
        "def pick_name_col(gdf, candidates):\n",
        "    for c in candidates:\n",
        "        if c in gdf.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "prov_name = pick_name_col(gprov_m, [\"NOMPRO\", \"DPA_DESPRO\", \"PROVINCIA\", \"NOMBRE\"])\n",
        "cant_name = pick_name_col(gcant_m, [\"DPA_DESCAN\", \"CANTON\", \"NOMBRE\"])\n",
        "\n",
        "# =========================\n",
        "# Construir mapa choropleth (paleta YlOrRd)\n",
        "# =========================\n",
        "def make_choropleth(gdf, key_col, metric, title, out_html, center=(-1.5, -78.4), zoom=6):\n",
        "    if metric not in gdf.columns:\n",
        "        raise ValueError(f\"No existe la mÃ©trica '{metric}' en el GeoDataFrame. Columnas disponibles: {list(gdf.columns)}\")\n",
        "\n",
        "    m = folium.Map(location=list(center), zoom_start=zoom, tiles=\"cartodbpositron\")\n",
        "\n",
        "    # Choropleth: paleta amarillo->naranja->rojo\n",
        "    folium.Choropleth(\n",
        "        geo_data=gdf.__geo_interface__,\n",
        "        data=gdf[[key_col, metric]],\n",
        "        columns=[key_col, metric],\n",
        "        key_on=f\"feature.properties.{key_col}\",\n",
        "        fill_color=\"YlOrRd\",          # <-- tu paleta\n",
        "        fill_opacity=0.80,\n",
        "        line_opacity=0.25,\n",
        "        nan_fill_color=\"#948F8D\",\n",
        "        nan_fill_opacity=0.4,\n",
        "        legend_name=title,\n",
        "    ).add_to(m)\n",
        "\n",
        "    # Tooltip con campos Ãºtiles\n",
        "    tooltip_fields = [key_col]\n",
        "    tooltip_aliases = [f\"{key_col}:\"]\n",
        "\n",
        "    # agrega nombre si existe\n",
        "    if key_col == \"CODPRO\" and prov_name:\n",
        "        tooltip_fields.insert(0, prov_name)\n",
        "        tooltip_aliases.insert(0, \"Provincia:\")\n",
        "    if key_col == \"DPA_CANTON\" and cant_name:\n",
        "        tooltip_fields.insert(0, cant_name)\n",
        "        tooltip_aliases.insert(0, \"CantÃ³n:\")\n",
        "\n",
        "    # mÃ©tricas\n",
        "    for f, a in [(\"mean_risk\", \"Riesgo promedio:\"), (\"pct_flag\", \"% marcados riesgo:\"), (\"n\", \"N:\")]:\n",
        "        if f in gdf.columns:\n",
        "            tooltip_fields.append(f)\n",
        "            tooltip_aliases.append(a)\n",
        "\n",
        "    folium.GeoJson(\n",
        "        gdf,\n",
        "        name=\"Datos\",\n",
        "        tooltip=GeoJsonTooltip(fields=tooltip_fields, aliases=tooltip_aliases, localize=True),\n",
        "        style_function=lambda x: {\"weight\": 0.6, \"color\": \"#666\", \"fillOpacity\": 0.0},\n",
        "        highlight_function=lambda x: {\"weight\": 2.0, \"color\": \"#111\"},\n",
        "    ).add_to(m)\n",
        "\n",
        "    folium.LayerControl().add_to(m)\n",
        "    m.save(out_html)\n",
        "    print(\"âœ… Guardado:\", out_html)\n",
        "    return m\n",
        "\n",
        "# =========================\n",
        "# Generar mapas (PROVINCIAS)\n",
        "# =========================\n",
        "m_prov_pct = make_choropleth(\n",
        "    gdf=gprov_m,\n",
        "    key_col=\"CODPRO\",\n",
        "    metric=\"pct_flag\",\n",
        "    title=\"Provincias - % marcados como riesgo (amarillo=bajo, rojo=alto)\",\n",
        "    out_html=str(OUT_DIR / \"mapa_provincias_pctflag_YlOrRd.html\"),\n",
        "    zoom=6\n",
        ")\n",
        "\n",
        "m_prov_mean = make_choropleth(\n",
        "    gdf=gprov_m,\n",
        "    key_col=\"CODPRO\",\n",
        "    metric=\"mean_risk\",\n",
        "    title=\"Provincias - Riesgo promedio (amarillo=bajo, rojo=alto)\",\n",
        "    out_html=str(OUT_DIR / \"mapa_provincias_meanrisk_YlOrRd.html\"),\n",
        "    zoom=6\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Generar mapas (CANTONES) - puede ser mÃ¡s pesado\n",
        "# =========================\n",
        "m_cant_pct = make_choropleth(\n",
        "    gdf=gcant_m,\n",
        "    key_col=\"DPA_CANTON\",\n",
        "    metric=\"pct_flag\",\n",
        "    title=\"Cantones - % marcados como riesgo (amarillo=bajo, rojo=alto)\",\n",
        "    out_html=str(OUT_DIR / \"mapa_cantones_pctflag_YlOrRd.html\"),\n",
        "    zoom=6\n",
        ")\n",
        "\n",
        "m_cant_mean = make_choropleth(\n",
        "    gdf=gcant_m,\n",
        "    key_col=\"DPA_CANTON\",\n",
        "    metric=\"mean_risk\",\n",
        "    title=\"Cantones - Riesgo promedio (amarillo=bajo, rojo=alto)\",\n",
        "    out_html=str(OUT_DIR / \"mapa_cantones_meanrisk_YlOrRd.html\"),\n",
        "    zoom=6\n",
        ")\n",
        "\n",
        "# Mostrar uno\n",
        "m_prov_pct\n"
      ],
      "metadata": {
        "id": "qoj48bJ7IJFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}